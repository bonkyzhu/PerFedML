server_config:
  setting: cross-device
  # learning strategy for serve: str; currently supported: {FeaAvg, FedDiscrete}
  strategy: None
  # number of communication roundsï¼š int
  num_rounds: 60
  # number of total participating clients: int
  num_clients: 10
  # control numper of participating clients per round: float [0, 1.0];
  participate_ratio: 1.0
  # randomly remove `drop_ratio` fraction of total participating clients at the aggregation time of each round: float [0.0, 1.0)
  drop_ratio: 0.0
  test_every: 1
  use_tqdm: False
  # dataset used for training: str; currently supported: {Mnist, FashionMnist, Cifar10}
  dataset: AGNews
  # dataset partition strategy: str; {'iid-equal-size', 'iid-diff-size', 'noniid-label-quantity', 'noniid-label-distribution', 'shards'}
  partition: None
  # float >0
  beta: None
  # int <= 10
  num_classes_per_client: None
  # int
  num_shards_per_client: None
  num_classes: 4
  learning_rate: 1.0
  lr_decay_per_round: 1.0
  # layers to be skipped in aggregation
  exclude: !!python/tuple []

  # algorithm specific settings go here
  # FedNH:
  smoothing: 0.9
  # FedDyn
  feddyn_alpha: 0.001
  # FedPHP
  fedphp_mu: 0.2
  fix_mu: True


client_config:
  # network used for each client
  model: LSTM
  embedding_size: 128
  hidden_size: 64
  # network specific setting goes here

  # dataset setting
  dataset: AGNews
  # input size: size of a single input; 3 channel, 32*32
  input_size: !!python/tuple [3, 32, 32]
  num_classes: 4
  # number of local epochs: int
  num_epochs: 5
  # number of samples per batch: int
  batch_size: 64
  # {Adam, SGD}
  optimizer: Adam
  # initial learning rate for each round
  learning_rate: 0.001
  lr_scheduler: stepwise
  lr_decay_per_round: 0.99
  num_rounds: 60
  participate_ratio: 1.0
  # other settings
  use_tqdm: False
  return_embedding: False

  # algorithm specific settings go here

  normalize: None # {FedAvg:False, pFedUH:True}
  # pFedUH settings
  head_init: orthogonal
  fix_scaling: False
  # Ditto
  lam: 0.75  # penalty parameter for Ditto follows the setting of FedRep

  # pFedMe:
  lambda_pfedme: 15
  local_steps_pfedme: 5
  # pmodel_lr and lgmodel_lr is the learning rate for the global model and personalized model on the client side
  # in the actual test I set both of them to be the same and derive them from the setup_optimizer function to make fare comparison
  pmodel_lr: 0.05
  lgmodel_lr: 0.05
  # FedRep:
  head_finetune_round: 2
  # FedProx
  mu: 0.001
  # APFL
  apfl_alpha: 0.25
  # FedDyn
  feddyn_alpha: 0.001
  # FedPHP
  fedphp_mu: 0.2
  fix_mu: True

  